I’m working on a Python repository that implements a command-line interface (CLI) tool designed to orchestrate local agents that perform deep research tasks using various large language model (LLM) providers. The goal of the tool is to give users a local-first, extensible way to run intelligent agents that can collaborate on complex research workflows using models like OpenAI’s GPT, local models via tools like Ollama or LM Studio, and other pluggable providers.

This CLI tool is already functional, but I’m preparing it for a multi-step refactoring process using Cursor AI. Before I start refactoring, I need to collect high-quality research on best practices for building this kind of application — both from the perspective of modern Python CLI development and from the perspective of LLM agent orchestration architecture.

Your task is to conduct research that will help inform how this tool should be structured, extended, and maintained going forward. Focus on identifying relevant best practices, patterns, and pitfalls. You should decide how to structure the research, what questions to pursue, and what sources to consult. The results of your research will be used to guide the refactor of the codebase.